{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleTransfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP_65MGkWCgZ",
        "colab_type": "text"
      },
      "source": [
        "## **STYLE** **TRANSFER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US9YxuijWLOi",
        "colab_type": "text"
      },
      "source": [
        "Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.\n",
        "\n",
        "This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usIyVJt-WXi3",
        "colab_type": "text"
      },
      "source": [
        "#INSTALLING THE LIBRARIES\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk2w27qeWZLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJR4zJlMXK6t",
        "colab_type": "text"
      },
      "source": [
        "wWe have used vgg 19 as the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBqpdSHdWFy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# define the VGG\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        # load the vgg model's features\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "    \n",
        "    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "            Extracts the features for the content loss from the block4_conv2 of VGG19\n",
        "            Args:\n",
        "                x: torch.Tensor - input image we want to extract the features of\n",
        "            Returns:\n",
        "                features: torch.Tensor - the activation maps of the block4_conv2 layer\n",
        "        \"\"\"\n",
        "        features = self.vgg[:23](x)\n",
        "        return features\n",
        "    \n",
        "    def get_style_activations(self, x):\n",
        "        \"\"\"\n",
        "            Extracts the features for the style loss from the block1_conv1, \n",
        "                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n",
        "            Args:\n",
        "                x: torch.Tensor - input image we want to extract the features of\n",
        "            Returns:\n",
        "                features: list - the list of activation maps of the block1_conv1, \n",
        "                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n",
        "        \"\"\"\n",
        "        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
        "        return features\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vtp8AxAXTwp",
        "colab_type": "text"
      },
      "source": [
        "Defining the losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH6fakD0LBOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram(tensor):\n",
        "    \"\"\"\n",
        "        Constructs the Gramian matrix out of the tensor\n",
        "    \"\"\"\n",
        "    return torch.mm(tensor, tensor.t())\n",
        "\n",
        "\n",
        "def gram_loss(noise_img_gram, style_img_gram, N, M):\n",
        "    \"\"\"\n",
        "        Gramian loss: the SSE between Gramian matrices of a layer\n",
        "            arXiv:1508.06576v2 - equation (4)\n",
        "    \"\"\"\n",
        "    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n",
        "\n",
        "\n",
        "def total_variation_loss(image):\n",
        "    \"\"\"\n",
        "        Variation loss makes the images smoother, defined over spacial dimensions\n",
        "    \"\"\"\n",
        "    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
        "        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def content_loss(noise: torch.Tensor, image: torch.Tensor):\n",
        "    \"\"\"\n",
        "        Simple SSE loss over the generated image and the content image\n",
        "            arXiv:1508.06576v2 - equation (1)\n",
        "    \"\"\"\n",
        "    return 1/2. * torch.sum(torch.pow(noise - image, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevv1Tp2Ln1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(style_img_path: str,\n",
        "         content_img_path: str, \n",
        "         img_dim: int,\n",
        "         num_iter: int,\n",
        "         style_weight: int,\n",
        "         content_weight: int,\n",
        "         variation_weight: int,\n",
        "         print_every: int,\n",
        "         save_every: int):\n",
        "\n",
        "    assert style_img_path is not None\n",
        "    assert content_img_path is not None\n",
        "\n",
        "    # define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # read the images\n",
        "    style_img = Image.open(style_img_path)\n",
        "    cont_img = Image.open(content_img_path)\n",
        "    \n",
        "    # define the transform\n",
        "    transform = transforms.Compose([transforms.Resize((img_dim, img_dim)),\n",
        "                                    transforms.ToTensor(), \n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                         [0.229, 0.224, 0.225])])\n",
        "    \n",
        "    # get the tensor of the image\n",
        "    content_image = transform(cont_img).unsqueeze(0).to(device)\n",
        "    style_image = transform(style_img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # init the network\n",
        "    vgg = VGG().to(device).eval()\n",
        "    \n",
        "    # replace the MaxPool with the AvgPool layers\n",
        "    for name, child in vgg.vgg.named_children():\n",
        "        if isinstance(child, nn.MaxPool2d):\n",
        "            vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            \n",
        "    # lock the gradients\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # get the content activations of the content image and detach them from the graph\n",
        "    content_activations = vgg.get_content_activations(content_image).detach()\n",
        "    \n",
        "    # unroll the content activations\n",
        "    content_activations = content_activations.view(512, -1)\n",
        "    \n",
        "    # get the style activations of the style image\n",
        "    style_activations = vgg.get_style_activations(style_image)\n",
        "    \n",
        "    # for every layer in the style activations\n",
        "    for i in range(len(style_activations)):\n",
        "\n",
        "        # unroll the activations and detach them from the graph\n",
        "        style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n",
        "\n",
        "    # calculate the gram matrices of the style image\n",
        "    style_grams = [gram(style_activations[i]) for i in range(len(style_activations))]\n",
        "    \n",
        "    # generate the Gaussian noise\n",
        "    noise = torch.randn(1, 3, img_dim, img_dim, device=device, requires_grad=True)\n",
        "    \n",
        "    # define the adam optimizer\n",
        "    # pass the feature map pixels to the optimizer as parameters\n",
        "    adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    # run the iteration\n",
        "    for iteration in range(num_iter):\n",
        "\n",
        "        # zero the gradient\n",
        "        adam.zero_grad()\n",
        "\n",
        "        # get the content activations of the Gaussian noise\n",
        "        noise_content_activations = vgg.get_content_activations(noise)\n",
        "\n",
        "        # unroll the feature maps of the noise\n",
        "        noise_content_activations = noise_content_activations.view(512, -1)\n",
        "\n",
        "        # calculate the content loss\n",
        "        content_loss_ = content_loss(noise_content_activations, content_activations)\n",
        "\n",
        "        # get the style activations of the noise image\n",
        "        noise_style_activations = vgg.get_style_activations(noise)\n",
        "\n",
        "        # for every layer\n",
        "        for i in range(len(noise_style_activations)):\n",
        "\n",
        "            # unroll the the noise style activations\n",
        "            noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n",
        "\n",
        "        # calculate the noise gram matrices\n",
        "        noise_grams = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n",
        "\n",
        "        # calculate the total weighted style loss\n",
        "        style_loss = 0\n",
        "        for i in range(len(style_activations)):\n",
        "            N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n",
        "            style_loss += (gram_loss(noise_grams[i], style_grams[i], N, M) / 5.)\n",
        "\n",
        "        # put the style loss on device\n",
        "        style_loss = style_loss.to(device)\n",
        "            \n",
        "        # calculate the total variation loss\n",
        "        variation_loss = total_variation_loss(noise).to(device)\n",
        "\n",
        "        # weight the final losses and add them together\n",
        "        total_loss = content_weight * content_loss_ + style_weight * style_loss + variation_weight * variation_loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n",
        "                                                                                                     content_weight * content_loss_.item(),\n",
        "                                                                                                     style_weight * style_loss.item(), \n",
        "                                                                                                     variation_weight * variation_loss.item()))\n",
        "\n",
        "        # create the folder for the generated images\n",
        "        if not os.path.exists('./generated/'):\n",
        "            os.mkdir('./generated/')\n",
        "        \n",
        "        # generate the image\n",
        "        if iteration % save_every == 0:\n",
        "            save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n",
        "\n",
        "        # backprop\n",
        "        total_loss.backward()\n",
        "        \n",
        "        # update parameters\n",
        "        adam.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaRtt96IMkPk",
        "colab_type": "code",
        "outputId": "9f400d84-32be-4bc5-e267-7ca820070127",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-77197679-8926-4c38-aaf7-ab58d0e88ded\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-77197679-8926-4c38-aaf7-ab58d0e88ded\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving content_img_1.jpg to content_img_1.jpg\n",
            "Saving style_img_1.jpeg to style_img_1.jpeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9uOVjojNlwI",
        "colab_type": "code",
        "outputId": "c3560e92-fd8c-41b3-dd3a-56b6acc0111a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "content_img_1.jpg  sample_data\tstyle_img_1.jpeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PflNHxPPAW3",
        "colab_type": "code",
        "outputId": "2d608fa3-ea39-4a36-ce71-c5729855d798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "style_img = '/content/style_img_1.jpeg'\n",
        "content_img = '/content/content_img_1.jpg'\n",
        "\n",
        "main(style_img, content_img, 512, 12000, 10e6, 10e-4, 10e3, 500, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 574673361/574673361 [00:39<00:00, 14384450.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Content Loss: 414.925, Style Loss: 3903571.367, Var Loss: 22584.934\n",
            "Iteration: 500, Content Loss: 934.210, Style Loss: 142888.492, Var Loss: 18369.811\n",
            "Iteration: 1000, Content Loss: 982.677, Style Loss: 58593.592, Var Loss: 16774.299\n",
            "Iteration: 1500, Content Loss: 966.612, Style Loss: 16338.299, Var Loss: 14427.347\n",
            "Iteration: 2000, Content Loss: 944.212, Style Loss: 8607.111, Var Loss: 11357.429\n",
            "Iteration: 2500, Content Loss: 920.634, Style Loss: 5771.374, Var Loss: 8370.676\n",
            "Iteration: 3000, Content Loss: 901.069, Style Loss: 4219.564, Var Loss: 5931.509\n",
            "Iteration: 3500, Content Loss: 883.258, Style Loss: 3204.975, Var Loss: 4302.669\n",
            "Iteration: 4000, Content Loss: 865.142, Style Loss: 2477.432, Var Loss: 3404.610\n",
            "Iteration: 4500, Content Loss: 842.117, Style Loss: 1941.018, Var Loss: 2951.732\n",
            "Iteration: 5000, Content Loss: 820.310, Style Loss: 1533.913, Var Loss: 2709.621\n",
            "Iteration: 5500, Content Loss: 800.971, Style Loss: 1222.754, Var Loss: 2559.505\n",
            "Iteration: 6000, Content Loss: 783.805, Style Loss: 988.028, Var Loss: 2454.072\n",
            "Iteration: 6500, Content Loss: 768.020, Style Loss: 807.935, Var Loss: 2374.009\n",
            "Iteration: 7000, Content Loss: 753.489, Style Loss: 670.455, Var Loss: 2309.920\n",
            "Iteration: 7500, Content Loss: 739.412, Style Loss: 567.588, Var Loss: 2255.641\n",
            "Iteration: 8000, Content Loss: 727.487, Style Loss: 496.843, Var Loss: 2209.293\n",
            "Iteration: 8500, Content Loss: 716.858, Style Loss: 448.452, Var Loss: 2171.621\n",
            "Iteration: 9000, Content Loss: 707.886, Style Loss: 415.102, Var Loss: 2141.980\n",
            "Iteration: 9500, Content Loss: 701.111, Style Loss: 391.921, Var Loss: 2119.589\n",
            "Iteration: 10000, Content Loss: 694.625, Style Loss: 376.234, Var Loss: 2102.657\n",
            "Iteration: 10500, Content Loss: 689.366, Style Loss: 362.783, Var Loss: 2088.631\n",
            "Iteration: 11000, Content Loss: 684.444, Style Loss: 355.526, Var Loss: 2078.474\n",
            "Iteration: 11500, Content Loss: 680.367, Style Loss: 349.319, Var Loss: 2069.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWazIvW21brs",
        "colab_type": "code",
        "outputId": "e314c4c4-86f6-4285-b055-2c4a209c8547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!ls /content/generated/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter_0.png\titer_11000.png\titer_4000.png  iter_7000.png\n",
            "iter_10000.png\titer_2000.png\titer_5000.png  iter_8000.png\n",
            "iter_1000.png\titer_3000.png\titer_6000.png  iter_9000.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8HCffMtXgFS",
        "colab_type": "text"
      },
      "source": [
        "#Downloading one of the generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9TkhXjr1nYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/generated/iter_8000.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxrtQfxwNcQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}